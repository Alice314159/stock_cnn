# -*- coding: utf-8 -*-
# enhanced_inception_stock_cnn.py
"""
Enhanced Inception Stock CNN with:
1. Inceptionå¤šå°ºåº¦æ¶æ„ - å¹¶è¡Œæ•è·ä¸åŒæ—¶é—´å°ºåº¦æ¨¡å¼
2. Volume-based features - å…³é”®çš„æˆäº¤é‡ç‰¹å¾
3. Adaptive threshold labeling - è‡ªé€‚åº”é˜ˆå€¼æ ‡ç­¾
4. Loguruæ—¥å¿—ç³»ç»Ÿ - æ›´å¥½çš„æ—¥å¿—è¾“å‡º
5. å…¨é¢çš„æ•°æ®è´¨é‡åˆ†æ
"""

import random, warnings
from dataclasses import dataclass
from typing import List, Optional, Tuple, Dict
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from torch.optim.lr_scheduler import ReduceLROnPlateau
import torch.nn.functional as F

from sklearn.preprocessing import RobustScaler, MinMaxScaler
from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix
from sklearn.metrics import precision_recall_fscore_support
from sklearn.utils.class_weight import compute_class_weight

from loguru import logger
import sys
import os

warnings.filterwarnings("ignore")


# =========================
# LOGURU é…ç½®
# =========================
def setup_logger():
    """é…ç½®loguruæ—¥å¿—ç³»ç»Ÿ"""
    # ç§»é™¤é»˜è®¤å¤„ç†å™¨
    logger.remove()

    # æ·»åŠ æ§åˆ¶å°è¾“å‡º
    logger.add(
        sys.stderr,
        format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>",
        level="INFO"
    )

    # æ·»åŠ æ–‡ä»¶è¾“å‡º
    log_file = "stock_cnn_training.log"
    logger.add(
        log_file,
        format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function}:{line} - {message}",
        level="DEBUG",
        rotation="10 MB",
        retention="7 days",
        compression="zip"
    )

    logger.info("ğŸš€ Logger initialized successfully")
    return log_file


def set_seed(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    logger.info(f"ğŸ² Random seed set to {seed}")


def device_auto():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info(f"ğŸ–¥ï¸  Using device: {device}")
    return device


# =========================
# DATA LOADING (ä¿æŒåŸæœ‰åŠŸèƒ½)
# =========================
def _coerce_numeric(df: pd.DataFrame, cols: List[str]) -> pd.DataFrame:
    """æŠŠå­—ç¬¦ä¸²æ•°å€¼ï¼ˆå«é€—å·ã€ç§‘å­¦è®¡æ•°æ³•ï¼‰è½¬æˆæ•°å€¼ï¼›æ— æ³•è§£æå˜ NaNã€‚"""
    for c in cols:
        if c in df.columns:
            df[c] = (
                df[c].astype(str)
                .str.replace(",", "", regex=False)
                .str.strip()
            )
            df[c] = pd.to_numeric(df[c], errors="coerce")
    return df


def adapt_columns(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()

    def norm(s: str) -> str:
        return (s.strip().lower()
                .replace(" ", "")
                .replace("_", "")
                .replace("(", "")
                .replace(")", "")
                .replace("%", "")
                .replace("-", "")
                .replace("ï¼", "/"))

    df.columns = [norm(c) for c in df.columns]

    alias = {
        "tradedate": "date", "date": "date",
        "secucode": "asset", "stockcode": "asset", "tscode": "asset",
        "open": "open", "high": "high", "low": "low", "close": "close", "volume": "volume",
        "transactionamount": "turnover_value", "turnovervalue": "turnover_value",
        "amplitude": "amplitude",
        "turnoverrate": "turnover_rate",
        "changepercentage": "chg_pct", "changepercent": "chg_pct",
        "changeamount": "chg_amt"
    }
    df.columns = [alias.get(c, c) for c in df.columns]

    required = ["date", "open", "high", "low", "close", "volume"]
    miss = [c for c in required if c not in df.columns]
    if miss:
        logger.error(f"âŒ Missing required columns: {miss}")
        raise ValueError(f"Missing required columns: {miss}")

    raw_date = (
        df["date"].astype(str)
        .str.replace("\ufeff", "", regex=False)
        .str.replace("\u200b", "", regex=False)
        .str.replace("\u00a0", "", regex=False)
        .str.replace("ï¼", "/", regex=False)
        .str.replace("å¹´", "/", regex=False)
        .str.replace("æœˆ", "/", regex=False)
        .str.replace("æ—¥", "", regex=False)
        .str.replace(".", "/", regex=False)
        .str.replace("-", "/", regex=False)
        .str.strip()
    )

    date_parsed = pd.to_datetime(raw_date, errors="coerce", format="%m/%d/%Y")
    mask = date_parsed.isna()
    if mask.any():
        date_parsed.loc[mask] = pd.to_datetime(raw_date[mask], errors="coerce", format="%Y/%m/%d")
        mask = date_parsed.isna()
    if mask.any():
        date_parsed.loc[mask] = pd.to_datetime(raw_date[mask], errors="coerce", format="%d/%m/%Y")
        mask = date_parsed.isna()
    if mask.any():
        date_parsed.loc[mask] = pd.to_datetime(raw_date[mask], errors="coerce", infer_datetime_format=True)
    df["date"] = date_parsed

    df = df.dropna(subset=["date"]).sort_values("date").reset_index(drop=True)

    numeric_cols = ["open", "high", "low", "close", "volume",
                    "turnover_value", "amplitude", "turnover_rate", "chg_pct", "chg_amt"]
    df = _coerce_numeric(df, [c for c in numeric_cols if c in df.columns])

    for c in ["turnover_value", "amplitude", "turnover_rate", "chg_pct", "chg_amt"]:
        if c in df.columns and df[c].isna().all():
            df.drop(columns=[c], inplace=True)

    return df


def load_generic_market_csv(file_path: str, asset_code: Optional[str] = None) -> pd.DataFrame:
    logger.info(f"ğŸ“Š Loading data from: {file_path}")
    try:
        df = pd.read_csv(file_path, dtype=str)
        df = adapt_columns(df)
        if asset_code is not None and "asset" in df.columns:
            df = df[df["asset"].astype(str) == str(asset_code)].copy()
        logger.success(f"âœ… Successfully loaded {len(df)} rows with columns: {list(df.columns)}")
        return df
    except Exception as e:
        logger.error(f"âŒ Failed to load data: {e}")
        raise


# =========================
# INCEPTION MODULE ARCHITECTURE
# =========================
class StockInceptionModule(nn.Module):
    """
    ä¸“é—¨ä¸ºè‚¡ç¥¨æ•°æ®è®¾è®¡çš„Inceptionæ¨¡å—
    å¹¶è¡Œä½¿ç”¨ä¸åŒå°ºå¯¸çš„å·ç§¯æ ¸æ•è·å¤šæ—¶é—´å°ºåº¦æ¨¡å¼
    """

    def __init__(self, in_channels: int, reduce_channels: int = 16, dropout: float = 0.1):
        super().__init__()

        # Branch 1: 1x1å·ç§¯ - æ•è·å½“å‰æ—¶ç‚¹ç‰¹å¾
        self.branch1 = nn.Sequential(
            nn.Conv2d(in_channels, reduce_channels, kernel_size=1),
            nn.BatchNorm2d(reduce_channels),
            nn.ReLU()
        )

        # Branch 2: 1x1é™ç»´ + 3x1å·ç§¯ - 3å¤©æ¨¡å¼
        self.branch2 = nn.Sequential(
            nn.Conv2d(in_channels, reduce_channels, kernel_size=1),
            nn.BatchNorm2d(reduce_channels),
            nn.ReLU(),
            nn.Conv2d(reduce_channels, reduce_channels * 2, kernel_size=(3, 1), padding=(1, 0)),
            nn.BatchNorm2d(reduce_channels * 2),
            nn.ReLU()
        )

        # Branch 3: 1x1é™ç»´ + 5x1å·ç§¯ - 5å¤©æ¨¡å¼
        self.branch3 = nn.Sequential(
            nn.Conv2d(in_channels, reduce_channels, kernel_size=1),
            nn.BatchNorm2d(reduce_channels),
            nn.ReLU(),
            nn.Conv2d(reduce_channels, reduce_channels * 2, kernel_size=(5, 1), padding=(2, 0)),
            nn.BatchNorm2d(reduce_channels * 2),
            nn.ReLU()
        )

        # Branch 4: 1x1é™ç»´ + 7x1å·ç§¯ - 7å¤©æ¨¡å¼
        self.branch4 = nn.Sequential(
            nn.Conv2d(in_channels, reduce_channels, kernel_size=1),
            nn.BatchNorm2d(reduce_channels),
            nn.ReLU(),
            nn.Conv2d(reduce_channels, reduce_channels * 2, kernel_size=(7, 1), padding=(3, 0)),
            nn.BatchNorm2d(reduce_channels * 2),
            nn.ReLU()
        )

        # Branch 5: MaxPooling + 1x1å·ç§¯ - ä¿ç•™é‡è¦ç‰¹å¾
        self.branch5 = nn.Sequential(
            nn.MaxPool2d(kernel_size=(3, 1), stride=1, padding=(1, 0)),
            nn.Conv2d(in_channels, reduce_channels, kernel_size=1),
            nn.BatchNorm2d(reduce_channels),
            nn.ReLU()
        )

        # ç‰¹å¾æ³¨æ„åŠ›æœºåˆ¶
        total_channels = reduce_channels + reduce_channels * 2 * 3 + reduce_channels  # 1+2+2+2+1å€
        self.channel_attention = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(total_channels, total_channels // 4, 1),
            nn.ReLU(),
            nn.Conv2d(total_channels // 4, total_channels, 1),
            nn.Sigmoid()
        )

        self.dropout = nn.Dropout2d(dropout)

    def forward(self, x):
        # å¹¶è¡Œè®¡ç®—ä¸åŒåˆ†æ”¯
        branch1_out = self.branch1(x)
        branch2_out = self.branch2(x)
        branch3_out = self.branch3(x)
        branch4_out = self.branch4(x)
        branch5_out = self.branch5(x)

        # è¿æ¥æ‰€æœ‰åˆ†æ”¯è¾“å‡º
        concatenated = torch.cat([branch1_out, branch2_out, branch3_out, branch4_out, branch5_out], dim=1)

        # åº”ç”¨é€šé“æ³¨æ„åŠ›
        attention_weights = self.channel_attention(concatenated)
        attended_features = concatenated * attention_weights

        # Dropoutæ­£åˆ™åŒ–
        output = self.dropout(attended_features)

        return output


class AdvancedStockInceptionModule(nn.Module):
    """
    å¢å¼ºç‰ˆInceptionæ¨¡å—ï¼Œä½¿ç”¨è†¨èƒ€å·ç§¯æ‰©å¤§æ„Ÿå—é‡
    èƒ½å¤Ÿæ•è·æ›´é•¿æœŸçš„ä¾èµ–å…³ç³»ï¼ˆ10å¤©ã€15å¤©ã€20å¤©æ¨¡å¼ï¼‰
    """

    def __init__(self, in_channels: int, reduce_channels: int = 16, dropout: float = 0.1):
        super().__init__()

        # Branch 1: çŸ­æœŸæ¨¡å¼ (1-3å¤©)
        self.short_term = nn.Sequential(
            nn.Conv2d(in_channels, reduce_channels, kernel_size=1),
            nn.BatchNorm2d(reduce_channels),
            nn.ReLU(),
            nn.Conv2d(reduce_channels, reduce_channels * 2, kernel_size=(3, 1), padding=(1, 0)),
            nn.BatchNorm2d(reduce_channels * 2),
            nn.ReLU()
        )

        # Branch 2: ä¸­æœŸæ¨¡å¼ (5-7å¤©) - ä½¿ç”¨è†¨èƒ€å·ç§¯
        self.medium_term = nn.Sequential(
            nn.Conv2d(in_channels, reduce_channels, kernel_size=1),
            nn.BatchNorm2d(reduce_channels),
            nn.ReLU(),
            nn.Conv2d(reduce_channels, reduce_channels * 2, kernel_size=(3, 1),
                      dilation=(2, 1), padding=(2, 0)),  # è†¨èƒ€ç‡=2ï¼Œç­‰æ•ˆ5å¤©æ„Ÿå—é‡
            nn.BatchNorm2d(reduce_channels * 2),
            nn.ReLU()
        )

        # Branch 3: é•¿æœŸæ¨¡å¼ (10-15å¤©)
        self.long_term = nn.Sequential(
            nn.Conv2d(in_channels, reduce_channels, kernel_size=1),
            nn.BatchNorm2d(reduce_channels),
            nn.ReLU(),
            nn.Conv2d(reduce_channels, reduce_channels * 2, kernel_size=(3, 1),
                      dilation=(4, 1), padding=(4, 0)),  # è†¨èƒ€ç‡=4ï¼Œç­‰æ•ˆ9å¤©æ„Ÿå—é‡
            nn.BatchNorm2d(reduce_channels * 2),
            nn.ReLU()
        )

        # Branch 4: è¶…é•¿æœŸæ¨¡å¼ (20å¤©+)
        self.extra_long_term = nn.Sequential(
            nn.Conv2d(in_channels, reduce_channels, kernel_size=1),
            nn.BatchNorm2d(reduce_channels),
            nn.ReLU(),
            nn.Conv2d(reduce_channels, reduce_channels * 2, kernel_size=(3, 1),
                      dilation=(8, 1), padding=(8, 0)),  # è†¨èƒ€ç‡=8ï¼Œç­‰æ•ˆ17å¤©æ„Ÿå—é‡
            nn.BatchNorm2d(reduce_channels * 2),
            nn.ReLU()
        )

        # è·¨åˆ†æ”¯ç‰¹å¾äº¤äº’
        total_channels = reduce_channels * 2 * 4
        self.cross_branch_conv = nn.Sequential(
            nn.Conv2d(total_channels, reduce_channels * 4, kernel_size=1),
            nn.BatchNorm2d(reduce_channels * 4),
            nn.ReLU()
        )

        self.dropout = nn.Dropout2d(dropout)

    def forward(self, x):
        # è®¡ç®—å„åˆ†æ”¯è¾“å‡º
        short = self.short_term(x)
        medium = self.medium_term(x)
        long = self.long_term(x)
        extra_long = self.extra_long_term(x)

        # è¿æ¥æ‰€æœ‰åˆ†æ”¯
        concatenated = torch.cat([short, medium, long, extra_long], dim=1)

        # è·¨åˆ†æ”¯ç‰¹å¾äº¤äº’
        output = self.cross_branch_conv(concatenated)
        output = self.dropout(output)

        return output


# =========================
# ENHANCED INCEPTION CNN WITH VOLUME FEATURES
# =========================
class InceptionStockCNN(nn.Module):
    """
    å®Œæ•´çš„åŸºäºInceptionçš„è‚¡ç¥¨é¢„æµ‹CNN
    å¤šå±‚Inceptionæ¨¡å— + æ¸è¿›å¼ç‰¹å¾æå– + ç½®ä¿¡åº¦ä¼°è®¡
    """

    def __init__(self,
                 input_features: int,
                 n_classes: int = 2,
                 inception_layers: int = 3,
                 base_channels: int = 32,
                 dropout: float = 0.2):
        super().__init__()

        logger.info(f"ğŸ—ï¸  Building Inception CNN: {inception_layers} layers, {base_channels} base channels")

        # åˆå§‹ç‰¹å¾æå–
        self.initial_conv = nn.Sequential(
            nn.Conv2d(1, base_channels, kernel_size=(7, 3), padding=(3, 1)),
            nn.BatchNorm2d(base_channels),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1)),
            nn.Dropout2d(dropout * 0.5)
        )

        # å¤šå±‚Inceptionæ¨¡å—
        self.inception_layers = nn.ModuleList()
        current_channels = base_channels

        for i in range(inception_layers):
            if i < inception_layers - 1:
                # å‰é¢çš„å±‚ä½¿ç”¨åŸºç¡€Inception
                inception = StockInceptionModule(
                    current_channels,
                    reduce_channels=base_channels // 2,
                    dropout=dropout
                )
                # è®¡ç®—è¾“å‡ºé€šé“æ•°
                current_channels = base_channels // 2 + (base_channels // 2) * 2 * 3 + base_channels // 2
                logger.debug(f"ğŸ“Š Inception layer {i + 1}: {current_channels} output channels")
            else:
                # æœ€åä¸€å±‚ä½¿ç”¨å¢å¼ºInception
                inception = AdvancedStockInceptionModule(
                    current_channels,
                    reduce_channels=base_channels // 2,
                    dropout=dropout
                )
                current_channels = (base_channels // 2) * 4
                logger.debug(f"ğŸ“Š Advanced Inception layer {i + 1}: {current_channels} output channels")

            self.inception_layers.append(inception)

        # å…¨å±€ç‰¹å¾èšåˆ
        self.global_features = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Linear(current_channels, 256),
            nn.ReLU(),
            nn.Dropout(dropout)
        )

        # åˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(64, n_classes)
        )

        # ç½®ä¿¡åº¦ä¼°è®¡å¤´
        self.confidence_head = nn.Sequential(
            nn.Linear(256, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )

        # åˆå§‹åŒ–æƒé‡
        self._init_weights()

        total_params = sum(p.numel() for p in self.parameters())
        logger.info(f"ğŸ”§ Model created with {total_params:,} parameters")

    def _init_weights(self):
        """åˆå§‹åŒ–æ¨¡å‹æƒé‡"""
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

    def forward(self, x):
        # åˆå§‹å·ç§¯
        x = self.initial_conv(x)

        # é€å±‚é€šè¿‡Inceptionæ¨¡å—
        for i, inception in enumerate(self.inception_layers):
            x = inception(x)

        # å…¨å±€ç‰¹å¾æå–
        global_features = self.global_features(x)

        # åˆ†ç±»é¢„æµ‹
        logits = self.classifier(global_features)

        # ç½®ä¿¡åº¦è¯„ä¼°
        confidence = self.confidence_head(global_features)

        return logits, confidence


# =========================
# ENHANCED VOLUME FEATURE ENGINEERING
# =========================
def calculate_rsi(prices: pd.Series, window: int = 14) -> pd.Series:
    """è®¡ç®—RSIæŒ‡æ ‡"""
    delta = prices.diff()
    gain = delta.clip(lower=0.0)
    loss = (-delta).clip(lower=0.0)
    avg_gain = gain.ewm(alpha=1 / window, adjust=False).mean()
    avg_loss = loss.ewm(alpha=1 / window, adjust=False).mean()
    rs = avg_gain / (avg_loss + 1e-8)
    rsi = 100 - (100 / (1 + rs))
    return rsi.fillna(50.0)


def calculate_volume_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    å…³é”®çš„æˆäº¤é‡ç‰¹å¾å·¥ç¨‹ - è¿™äº›ç‰¹å¾å¯¹çŸ­æœŸé¢„æµ‹è‡³å…³é‡è¦
    """
    logger.info("ğŸ”§ Computing volume-based features...")

    # æˆäº¤é‡ç§»åŠ¨å¹³å‡
    df["volume_sma_5"] = df["volume"].rolling(5).mean()
    df["volume_sma_20"] = df["volume"].rolling(20).mean()
    df["volume_sma_50"] = df["volume"].rolling(50).mean()

    # æˆäº¤é‡æ¯”ç‡ (ç›¸å¯¹æˆäº¤é‡å¼ºåº¦)
    df["volume_ratio_5"] = df["volume"] / (df["volume_sma_5"] + 1e-8)
    df["volume_ratio_20"] = df["volume"] / (df["volume_sma_20"] + 1e-8)
    df["volume_ratio_50"] = df["volume"] / (df["volume_sma_50"] + 1e-8)

    # æˆäº¤é‡è¶‹åŠ¿
    df["volume_trend"] = df["volume_sma_5"] / (df["volume_sma_20"] + 1e-8)

    # ä»·é‡ç‰¹å¾ (èµ„é‡‘æµå‘è¿‘ä¼¼)
    df["price_volume"] = df["close"] * df["volume"]
    df["pv_sma_5"] = df["price_volume"].rolling(5).mean()
    df["pv_ratio"] = df["price_volume"] / (df["pv_sma_5"] + 1e-8)

    # æˆäº¤é‡åŠ æƒå¹³å‡ä»·æ ¼ (VWAP) è¿‘ä¼¼
    df["vwap_approx"] = (df["close"] * df["volume"]).rolling(20).sum() / (df["volume"].rolling(20).sum() + 1e-8)
    df["price_vwap_ratio"] = df["close"] / (df["vwap_approx"] + 1e-8)

    # æˆäº¤é‡æ¿€å¢æ£€æµ‹
    volume_std = df["volume"].rolling(20).std()
    df["volume_spike"] = (df["volume"] - df["volume_sma_20"]) / (volume_std + 1e-8)

    # ä»·é‡èƒŒç¦»æŒ‡æ ‡
    df["price_change"] = df["close"].pct_change()
    df["volume_change"] = df["volume"].pct_change()
    df["price_volume_divergence"] = df["price_change"] * df["volume_change"]

    logger.success("âœ… Volume features computed successfully")
    return df


def enhanced_build_dataframe(df: pd.DataFrame,
                             target_col: str = "close",
                             prediction_mode: str = "adaptive_classification",
                             base_features: Optional[List[str]] = None,
                             volatility_lookback: int = 20,
                             threshold_multiplier: float = 0.5) -> pd.DataFrame:
    """
    å¢å¼ºçš„ç‰¹å¾å·¥ç¨‹ï¼Œç»“åˆæˆäº¤é‡å’Œè‡ªé€‚åº”æ ‡ç­¾
    """
    logger.info(f"ğŸ”§ Starting enhanced feature engineering with {prediction_mode} mode")

    df = df.copy()
    if "date" in df.columns:
        df = df.sort_values("date").set_index("date")

    # åŸºç¡€ä»·æ ¼ç‰¹å¾
    df["returns"] = df[target_col].pct_change()
    df["log_returns"] = np.log(df[target_col] / df[target_col].shift(1))

    # ä»·æ ¼æŠ€æœ¯æŒ‡æ ‡
    df["sma_5"] = df[target_col].rolling(5).mean()
    df["sma_10"] = df[target_col].rolling(10).mean()
    df["sma_20"] = df[target_col].rolling(20).mean()
    df["ema_12"] = df[target_col].ewm(span=12).mean()
    df["ema_26"] = df[target_col].ewm(span=26).mean()

    # RSIæŒ‡æ ‡
    df["rsi"] = calculate_rsi(df[target_col], window=14)
    df["rsi_21"] = calculate_rsi(df[target_col], window=21)

    # MACDæŒ‡æ ‡
    df["macd"] = df["ema_12"] - df["ema_26"]
    df["macd_signal"] = df["macd"].ewm(span=9).mean()
    df["macd_histogram"] = df["macd"] - df["macd_signal"]

    # å¸ƒæ—å¸¦
    bb_middle = df[target_col].rolling(20).mean()
    bb_std = df[target_col].rolling(20).std()
    df["bb_upper"] = bb_middle + (bb_std * 2)
    df["bb_lower"] = bb_middle - (bb_std * 2)
    df["bb_position"] = (df[target_col] - df["bb_lower"]) / (df["bb_upper"] - df["bb_lower"] + 1e-8)

    # æ³¢åŠ¨ç‡
    df["volatility"] = df["returns"].rolling(10).std()
    df["volatility_20"] = df["returns"].rolling(20).std()

    # ä»·æ ¼åŠ¨é‡
    df["roc_5"] = df[target_col].pct_change(5)
    df["roc_10"] = df[target_col].pct_change(10)

    # æ—¥å†…ç‰¹å¾
    df["hl_ratio"] = (df["high"] - df["low"]) / (df[target_col] + 1e-8)
    df["oc_ratio"] = (df["open"] - df[target_col]) / (df[target_col] + 1e-8)

    # **æ·»åŠ å…³é”®çš„æˆäº¤é‡ç‰¹å¾**
    df = calculate_volume_features(df)

    # ä»·æ ¼æ¯”ç‡
    df["price_sma5_ratio"] = df[target_col] / (df["sma_5"] + 1e-8)
    df["price_sma20_ratio"] = df[target_col] / (df["sma_20"] + 1e-8)

    # === å¢å¼ºçš„æ ‡ç­¾ç­–ç•¥ ===
    future_return = df[target_col].shift(-1) / df[target_col] - 1

    if prediction_mode == "adaptive_classification":
        logger.info(f"ğŸ“Š Using adaptive classification with threshold_multiplier={threshold_multiplier}")
        # ä½¿ç”¨æ»šåŠ¨æ³¢åŠ¨ç‡åˆ›å»ºè‡ªé€‚åº”é˜ˆå€¼
        rolling_vol = df["returns"].rolling(volatility_lookback).std()
        up_threshold = threshold_multiplier * rolling_vol
        down_threshold = -threshold_multiplier * rolling_vol

        # åˆ›å»ºè‡ªé€‚åº”é˜ˆå€¼æ ‡ç­¾
        labels = pd.Series(index=df.index, dtype=float)
        labels[future_return > up_threshold] = 1  # å¼ºåŠ¿ä¸Šæ¶¨
        labels[future_return < down_threshold] = 0  # å¼ºåŠ¿ä¸‹è·Œ
        # ä¸­é—´èŒƒå›´ä¿æŒNaNï¼ˆè¿‡æ»¤æ‰ï¼‰

        df["target"] = labels
        valid_samples = labels.notna().sum()
        up_samples = (labels == 1).sum()
        down_samples = (labels == 0).sum()

        logger.info(f"ğŸ“ˆ Adaptive labeling results:")
        logger.info(f"   Valid samples: {valid_samples}")
        logger.info(f"   Up samples: {up_samples} ({up_samples / valid_samples * 100:.1f}%)")
        logger.info(f"   Down samples: {down_samples} ({down_samples / valid_samples * 100:.1f}%)")

    elif prediction_mode == "magnitude_regression":
        # é¢„æµ‹æ”¶ç›Šç‡å¹…åº¦ï¼ˆç»å¯¹å€¼ï¼‰
        df["target"] = np.abs(future_return)

    elif prediction_mode == "direction_magnitude":
        # åˆ›å»ºä¸¤ä¸ªç›®æ ‡ï¼šæ–¹å‘å’Œå¹…åº¦
        df["target_direction"] = (future_return > 0).astype(float)
        df["target_magnitude"] = np.abs(future_return)
        df["target"] = df["target_direction"]  # ä¸»è¦ç›®æ ‡

    else:  # simple_binary
        logger.info("ğŸ“Š Using simple binary classification")
        valid = df[target_col].notna() & df[target_col].shift(-1).notna()
        df["target"] = np.where(valid, (df[target_col].shift(-1) > df[target_col]).astype(float), np.nan)

    # å¢å¼ºçš„ç‰¹å¾é€‰æ‹©
    volume_features = [
        "volume_ratio_5", "volume_ratio_20", "volume_ratio_50",
        "volume_trend", "pv_ratio", "price_vwap_ratio", "volume_spike",
        "price_volume_divergence"
    ]

    price_features = [
        "returns", "log_returns",
        "sma_5", "sma_20", "rsi", "rsi_21",
        "macd", "macd_histogram", "bb_position",
        "volatility", "volatility_20",
        "roc_5", "roc_10", "hl_ratio", "oc_ratio",
        "price_sma5_ratio", "price_sma20_ratio"
    ]

    if base_features is None:
        base_features = price_features + volume_features

    # æ·»åŠ ä»»ä½•é¢å¤–çš„å¯ç”¨ç‰¹å¾
    for c in ["amplitude", "turnover_rate", "chg_pct"]:
        if c in df.columns and c not in base_features:
            base_features.append(c)

    # æœ€ç»ˆæ¸…ç†
    out = df[base_features + ["target"]].copy()
    out = out[~out["target"].isna()]  # ç§»é™¤æ²¡æœ‰æ ‡ç­¾çš„è¡Œ

    # å¡«å……NaNç‰¹å¾
    feat_cols = [c for c in out.columns if c != "target"]
    out[feat_cols] = out[feat_cols].replace([np.inf, -np.inf], np.nan).fillna(0.0)

    # æ ¹æ®æ¨¡å¼è½¬æ¢ç›®æ ‡
    if prediction_mode in ["adaptive_classification", "simple_binary", "direction_magnitude"]:
        out["target"] = out["target"].astype(int)

    logger.success(f"âœ… Feature engineering complete: {len(out)} samples, {len(feat_cols)} features")
    return out


# =========================
# DATA QUALITY ANALYSIS WITH LOGURU
# =========================
def analyze_data_quality(df: pd.DataFrame, target_col: str = "target"):
    """
    å…¨é¢çš„æ•°æ®è´¨é‡åˆ†æ
    """
    logger.info("ğŸ“Š Starting data quality analysis...")

    logger.info(f"Total samples: {len(df)}")
    logger.info(f"Features: {len([c for c in df.columns if c != target_col])}")
    logger.info(f"Date range: {df.index.min()} to {df.index.max()}")
    logger.info(f"Time span: {(df.index.max() - df.index.min()).days} days")

    # ç›®æ ‡åˆ†å¸ƒ
    if target_col in df.columns:
        target_counts = df[target_col].value_counts().sort_index()
        logger.info("ğŸ¯ Target distribution:")
        for val, count in target_counts.items():
            pct = count / len(df) * 100
            logger.info(f"   Class {val}: {count} ({pct:.1f}%)")

        if len(target_counts) == 2:
            imbalance = target_counts.max() / target_counts.min()
            logger.info(f"   Imbalance ratio: {imbalance:.2f}")
            if imbalance > 3:
                logger.warning(f"âš ï¸  High class imbalance detected: {imbalance:.2f}")

    # ç¼ºå¤±å€¼åˆ†æ
    missing = df.isnull().sum()
    if missing.sum() > 0:
        logger.warning("âŒ Missing values found:")
        for col in missing[missing > 0].index:
            pct = missing[col] / len(df) * 100
            logger.warning(f"   {col}: {missing[col]} ({pct:.1f}%)")
    else:
        logger.success("âœ… No missing values")

    # ç‰¹å¾é‡è¦æ€§ (ä¸ç›®æ ‡çš„ç›¸å…³æ€§)
    if target_col in df.columns:
        feature_cols = [c for c in df.columns if c != target_col]
        correlations = []

        for col in feature_cols:
            if df[col].dtype in ['float64', 'float32', 'int64', 'int32']:
                corr = abs(df[col].corr(df[target_col]))
                if not pd.isna(corr):
                    correlations.append((col, corr))

        correlations.sort(key=lambda x: x[1], reverse=True)
        logger.info("ğŸ” Top 10 most predictive features:")
        for i, (col, corr) in enumerate(correlations[:10], 1):
            logger.info(f"   {i:2d}. {col:<25}: {corr:.4f}")

    # æ•°æ®åˆ†å¸ƒæ£€æŸ¥
    feature_cols = [c for c in df.columns if c != target_col and c != 'date']

    # æ£€æŸ¥æ½œåœ¨é—®é¢˜
    issues = []
    for col in feature_cols:
        zero_pct = (df[col] == 0).sum() / len(df)
        if zero_pct > 0.5:
            issues.append(f"{col}: {zero_pct * 100:.1f}% zeros")
        if df[col].std() < 1e-6:
            issues.append(f"{col}: near-constant values")

    if issues:
        logger.warning("âš ï¸  Potential issues found:")
        for issue in issues:
            logger.warning(f"   - {issue}")
    else:
        logger.success("âœ… No obvious distribution issues")

    logger.success("ğŸ“Š Data quality analysis completed")


# =========================
# ENHANCED DATASET
# =========================
class HeatmapDataset(Dataset):
    """å¢å¼ºçš„æ•°æ®é›†ç±»ï¼Œæ”¯æŒæ•°æ®å¢å¼º"""

    def __init__(self, X_scaled: np.ndarray, y: np.ndarray, window: int, augment: bool = False):
        self.X, self.y = self._mkseq(X_scaled, y, window)
        self.augment = augment
        logger.info(f"ğŸ“¦ Dataset created: {len(self.y)} sequences, window_size={window}, augment={augment}")

    @staticmethod
    def _mkseq(X_scaled: np.ndarray, y: np.ndarray, W: int) -> Tuple[torch.Tensor, torch.Tensor]:
        Xs, Ys = [], []
        T = len(X_scaled)
        for i in range(W, T):
            Xs.append(X_scaled[i - W:i])
            Ys.append(int(y[i - 1]))
        Xs = np.asarray(Xs, dtype=np.float32)[:, None, :, :]  # æ·»åŠ é€šé“ç»´åº¦
        Ys = np.asarray(Ys, dtype=np.int64)
        return torch.from_numpy(Xs), torch.from_numpy(Ys)

    def _augment_sequence(self, x):
        """æ—¶é—´åºåˆ—æ•°æ®å¢å¼º"""
        if not self.augment or np.random.random() > 0.3:  # 30%æ¦‚ç‡å¢å¼º
            return x

        # æ·»åŠ å°å¹…éšæœºå™ªå£° (0.5% of std)
        noise_std = torch.std(x) * 0.005
        noise = torch.randn_like(x) * noise_std
        return x + noise

    def __len__(self):
        return self.y.shape[0]

    def __getitem__(self, i):
        x = self.X[i]
        if self.augment:
            x = self._augment_sequence(x)
        return x, self.y[i]


# =========================
# ENHANCED CONFIG
# =========================
@dataclass
class EnhancedInceptionConfig:
    """å¢å¼ºçš„é…ç½®ç±»"""
    # æ¨¡å‹æ¶æ„
    window_size: int = 30
    inception_layers: int = 3
    base_channels: int = 32
    dropout: float = 0.2

    # è®­ç»ƒå‚æ•°
    scaler_type: str = "robust"
    batch_size: int = 64
    epochs: int = 80
    lr: float = 3e-4
    weight_decay: float = 1e-4
    train_ratio: float = 0.7
    val_ratio: float = 0.15

    # æ ‡ç­¾ç­–ç•¥
    prediction_mode: str = "adaptive_classification"
    volatility_lookback: int = 20
    threshold_multiplier: float = 0.5

    # å…¶ä»–
    seed: int = 42
    use_class_weights: bool = True
    use_data_augmentation: bool = True
    early_stopping_patience: int = 15


# =========================
# ENHANCED PIPELINE CLASS
# =========================
class EnhancedInceptionStockCNN:
    """å¢å¼ºçš„Inceptionè‚¡ç¥¨CNNæµæ°´çº¿"""

    def __init__(self, cfg: EnhancedInceptionConfig, features: Optional[List[str]] = None):
        self.cfg = cfg
        self.features = features
        self.model: Optional[nn.Module] = None
        self.scaler = RobustScaler() if cfg.scaler_type == "robust" else MinMaxScaler()
        self.device = device_auto()

        logger.info(f"ğŸ—ï¸  Initialized pipeline with config: {cfg}")

    def prepare_dataframe(self, df: pd.DataFrame, target_col="close") -> pd.DataFrame:
        """å‡†å¤‡æ•°æ®æ¡†"""
        return enhanced_build_dataframe(
            df,
            target_col=target_col,
            base_features=self.features,
            prediction_mode=self.cfg.prediction_mode,
            volatility_lookback=self.cfg.volatility_lookback,
            threshold_multiplier=self.cfg.threshold_multiplier
        )

    @staticmethod
    def time_splits(index: pd.DatetimeIndex, train_ratio=0.7, val_ratio=0.15):
        """æ—¶é—´åºåˆ—åˆ†å‰²"""
        n = len(index)
        i_tr_end = int(n * train_ratio)
        i_va_end = int(n * (train_ratio + val_ratio))
        tr = np.arange(n) < i_tr_end
        va = (np.arange(n) >= i_tr_end) & (np.arange(n) < i_va_end)
        te = np.arange(n) >= i_va_end

        logger.info(f"ğŸ“Š Data splits: Train={tr.sum()}, Val={va.sum()}, Test={te.sum()}")
        return tr, va, te

    def fit(self, df_clean: pd.DataFrame):
        """è®­ç»ƒæ¨¡å‹"""
        set_seed(self.cfg.seed)
        logger.info("ğŸš€ Starting model training...")

        # æ•°æ®è´¨é‡åˆ†æ
        analyze_data_quality(df_clean)

        if len(df_clean) == 0:
            logger.error("âŒ Empty dataset after preprocessing!")
            raise ValueError("df_clean has 0 rows.")

        feats = [c for c in df_clean.columns if c != "target"]
        y_all = df_clean["target"].astype(int).values
        X_all = df_clean[feats].astype(np.float32).values
        idx = df_clean.index

        logger.info(f"ğŸ“Š Model input shape: {X_all.shape}")
        logger.info(f"ğŸ”§ Features ({len(feats)}): {feats[:10]}{'...' if len(feats) > 10 else ''}")

        u, c = np.unique(y_all, return_counts=True)
        logger.info(f"ğŸ¯ Final label distribution: {dict(zip(u.tolist(), c.tolist()))}")

        classes = np.unique(y_all)
        n_classes = int(classes.max()) + 1

        tr_m, va_m, te_m = self.time_splits(idx, self.cfg.train_ratio, self.cfg.val_ratio)

        # ä»…åœ¨è®­ç»ƒæ•°æ®ä¸Šæ‹Ÿåˆç¼©æ”¾å™¨
        logger.info("ğŸ“ Fitting scaler on training data...")
        self.scaler.fit(X_all[tr_m])
        X_tr = self.scaler.transform(X_all[tr_m])
        X_va = self.scaler.transform(X_all[va_m])
        X_te = self.scaler.transform(X_all[te_m])

        W = self.cfg.window_size
        ds_tr = HeatmapDataset(X_tr, y_all[tr_m], W, augment=self.cfg.use_data_augmentation)
        ds_va = HeatmapDataset(X_va, y_all[va_m], W, augment=False)
        ds_te = HeatmapDataset(X_te, y_all[te_m], W, augment=False)

        logger.info(f"ğŸ“¦ Dataset sizes after windowing: train={len(ds_tr)}, val={len(ds_va)}, test={len(ds_te)}")

        if len(ds_tr) == 0 or len(ds_va) == 0:
            logger.error("âŒ Insufficient data after windowing!")
            raise ValueError("Insufficient data after windowing")

        dl_tr = DataLoader(ds_tr, batch_size=self.cfg.batch_size, shuffle=True, drop_last=True)
        dl_va = DataLoader(ds_va, batch_size=self.cfg.batch_size, shuffle=False)
        dl_te = DataLoader(ds_te, batch_size=self.cfg.batch_size, shuffle=False)

        # åˆ›å»ºInceptionæ¨¡å‹
        self.model = InceptionStockCNN(
            input_features=len(feats),
            n_classes=n_classes,
            inception_layers=self.cfg.inception_layers,
            base_channels=self.cfg.base_channels,
            dropout=self.cfg.dropout
        ).to(self.device)

        # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        optimizer = AdamW(self.model.parameters(), lr=self.cfg.lr, weight_decay=self.cfg.weight_decay)
        scheduler = ReduceLROnPlateau(optimizer, mode="min", factor=0.5, patience=8, min_lr=1e-6)

        # ç±»åˆ«æƒé‡å¤„ç†ä¸å¹³è¡¡
        if self.cfg.use_class_weights:
            weights = compute_class_weight('balanced', classes=np.unique(ds_tr.y.numpy()), y=ds_tr.y.numpy())
            weights = torch.tensor(weights, dtype=torch.float32).to(self.device)
            logger.info(f"âš–ï¸  Using class weights: {weights.cpu().numpy()}")
        else:
            weights = None

        # è®­ç»ƒå¾ªç¯
        history = {"train_loss": [], "val_loss": [], "lr": [], "train_acc": [], "val_acc": []}
        best_val_loss = float('inf')
        patience_counter = 0

        logger.info(f"ğŸ‹ï¸  Starting training for {self.cfg.epochs} epochs...")

        for ep in range(1, self.cfg.epochs + 1):
            # è®­ç»ƒé˜¶æ®µ
            self.model.train()
            tr_loss = tr_correct = n_tr = 0

            for xb, yb in dl_tr:
                xb, yb = xb.to(self.device), yb.to(self.device)
                optimizer.zero_grad()

                logits, confidence = self.model(xb)

                if weights is not None:
                    loss = nn.functional.cross_entropy(logits, yb, weight=weights)
                else:
                    loss = nn.functional.cross_entropy(logits, yb)

                loss.backward()

                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

                optimizer.step()

                tr_loss += float(loss) * len(yb)
                tr_correct += (logits.argmax(1) == yb).sum().item()
                n_tr += len(yb)

            tr_loss /= max(1, n_tr)
            tr_acc = tr_correct / max(1, n_tr)

            # éªŒè¯é˜¶æ®µ
            self.model.eval()
            va_loss = va_correct = n_va = 0

            with torch.no_grad():
                for xb, yb in dl_va:
                    xb, yb = xb.to(self.device), yb.to(self.device)
                    logits, confidence = self.model(xb)

                    if weights is not None:
                        loss = nn.functional.cross_entropy(logits, yb, weight=weights)
                    else:
                        loss = nn.functional.cross_entropy(logits, yb)

                    va_loss += float(loss) * len(yb)
                    va_correct += (logits.argmax(1) == yb).sum().item()
                    n_va += len(yb)

            va_loss /= max(1, n_va)
            va_acc = va_correct / max(1, n_va)

            scheduler.step(va_loss)
            cur_lr = optimizer.param_groups[0]["lr"]

            history["train_loss"].append(tr_loss)
            history["val_loss"].append(va_loss)
            history["train_acc"].append(tr_acc)
            history["val_acc"].append(va_acc)
            history["lr"].append(cur_lr)

            # æ¯10è½®è®°å½•ä¸€æ¬¡è¯¦ç»†ä¿¡æ¯
            if ep % 10 == 0 or ep <= 5:
                logger.info(f"Epoch {ep:03d} | "
                            f"train_loss={tr_loss:.4f} train_acc={tr_acc:.3f} | "
                            f"val_loss={va_loss:.4f} val_acc={va_acc:.3f} | "
                            f"lr={cur_lr:.2e}")

            # æ—©åœ
            if va_loss < best_val_loss:
                best_val_loss = va_loss
                patience_counter = 0
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                torch.save(self.model.state_dict(), 'best_inception_model.pth')
            else:
                patience_counter += 1

            if patience_counter >= self.cfg.early_stopping_patience:
                logger.info(f"ğŸ›‘ Early stopping triggered at epoch {ep}")
                break

        # åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæµ‹è¯•
        if os.path.exists('best_inception_model.pth'):
            self.model.load_state_dict(torch.load('best_inception_model.pth'))
            logger.info("ğŸ“¥ Loaded best model for testing")

        # æµ‹è¯•è¯„ä¼°
        logger.info("ğŸ§ª Starting final evaluation...")
        y_true, y_pred, confidences = self.predict_loader_with_confidence(dl_te)
        acc = accuracy_score(y_true, y_pred)
        f1m = f1_score(y_true, y_pred, average="macro")

        logger.success("=" * 60)
        logger.success("ğŸ‰ FINAL TEST RESULTS")
        logger.success("=" * 60)
        logger.success(f"Test Accuracy: {acc:.4f}")
        logger.success(f"Test F1-macro: {f1m:.4f}")
        logger.info(f"Baseline (random): 0.500")
        improvement = (acc - 0.5) * 100
        logger.success(f"Improvement over random: {improvement:+.1f}%")

        # ç½®ä¿¡åº¦åˆ†æ
        avg_confidence = np.mean(confidences)
        logger.info(f"Average prediction confidence: {avg_confidence:.3f}")

        if acc > 0.52:
            logger.success("ğŸ‰ Excellent! Above 52% is very good for stock prediction!")
        elif acc > 0.51:
            logger.success("ğŸ‘ Good! Above 51% shows meaningful signal.")
        elif acc > 0.505:
            logger.info("ğŸ“ˆ Slight edge detected. Consider feature engineering.")
        else:
            logger.warning("ğŸ“Š No clear signal. Try different features or approach.")

        logger.info("ğŸ“‹ Detailed Classification Report:")
        print(classification_report(y_true, y_pred, digits=4))
        logger.info("ğŸ”¢ Confusion Matrix:")
        print(confusion_matrix(y_true, y_pred))

        # åˆ†æé¢„æµ‹æ¨¡å¼
        self.analyze_predictions(y_true, y_pred, confidences)

        logger.success("=" * 60)
        return history, (y_true, y_pred, confidences)

    @torch.no_grad()
    def predict_loader_with_confidence(self, loader: DataLoader) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """å¸¦ç½®ä¿¡åº¦çš„é¢„æµ‹"""
        self.model.eval()
        y_true, y_pred, confidences = [], [], []
        for xb, yb in loader:
            logits, confidence = self.model(xb.to(self.device))
            y_pred.extend(logits.argmax(1).cpu().numpy().tolist())
            y_true.extend(yb.numpy().tolist())
            confidences.extend(confidence.cpu().numpy().flatten().tolist())
        return np.array(y_true), np.array(y_pred), np.array(confidences)

    def analyze_predictions(self, y_true, y_pred, confidences):
        """åˆ†æé¢„æµ‹ç»“æœ"""
        logger.info("ğŸ” Analyzing prediction patterns...")

        # é¢„æµ‹åˆ†å¸ƒ
        pred_dist = pd.Series(y_pred).value_counts().sort_index()
        true_dist = pd.Series(y_true).value_counts().sort_index()

        logger.info("ğŸ“Š Prediction vs True Distribution:")
        for cls in sorted(set(y_true) | set(y_pred)):
            pred_count = pred_dist.get(cls, 0)
            true_count = true_dist.get(cls, 0)
            logger.info(f"   Class {cls}: Predicted {pred_count}, True {true_count}")

        # æŒ‰ç±»åˆ«çš„æ€§èƒ½
        precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred, average=None)

        logger.info("ğŸ“ˆ Per-class Performance:")
        for i, (p, r, f, s) in enumerate(zip(precision, recall, f1, support)):
            logger.info(f"   Class {i}: Precision={p:.3f}, Recall={r:.3f}, F1={f:.3f}, Support={s}")

        # ç½®ä¿¡åº¦åˆ†æ
        correct_mask = (y_true == y_pred)
        correct_conf = confidences[correct_mask].mean()
        incorrect_conf = confidences[~correct_mask].mean()

        logger.info(f"ğŸ¯ Confidence Analysis:")
        logger.info(f"   Correct predictions confidence: {correct_conf:.3f}")
        logger.info(f"   Incorrect predictions confidence: {incorrect_conf:.3f}")
        logger.info(f"   Confidence gap: {correct_conf - incorrect_conf:.3f}")


# =========================
# å¢å¼ºçš„ç»˜å›¾åŠŸèƒ½
# =========================
def plot_enhanced_inception_history(history: dict, save_path: str = "inception_training_history.png"):
    """å¢å¼ºçš„è®­ç»ƒå†å²å¯è§†åŒ–"""
    try:
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))

        # Lossæ›²çº¿
        axes[0, 0].plot(history["train_loss"], label="Train Loss", alpha=0.8, linewidth=2)
        axes[0, 0].plot(history["val_loss"], label="Val Loss", alpha=0.8, linewidth=2)
        axes[0, 0].set_title("ğŸ“‰ Loss Curves", fontsize=14, fontweight='bold')
        axes[0, 0].set_xlabel("Epoch")
        axes[0, 0].set_ylabel("Loss")
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)

        # å‡†ç¡®ç‡æ›²çº¿
        axes[0, 1].plot(history["train_acc"], label="Train Acc", alpha=0.8, linewidth=2)
        axes[0, 1].plot(history["val_acc"], label="Val Acc", alpha=0.8, linewidth=2)
        axes[0, 1].axhline(y=0.5, color='red', linestyle='--', alpha=0.7, label='Random Baseline')
        axes[0, 1].set_title("ğŸ“ˆ Accuracy Curves", fontsize=14, fontweight='bold')
        axes[0, 1].set_xlabel("Epoch")
        axes[0, 1].set_ylabel("Accuracy")
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)

        # å­¦ä¹ ç‡
        axes[0, 2].plot(history["lr"], color='orange', linewidth=2)
        axes[0, 2].set_title("ğŸ›ï¸  Learning Rate Schedule", fontsize=14, fontweight='bold')
        axes[0, 2].set_xlabel("Epoch")
        axes[0, 2].set_ylabel("Learning Rate")
        axes[0, 2].set_yscale('log')
        axes[0, 2].grid(True, alpha=0.3)

        # éªŒè¯å‡†ç¡®ç‡è¶‹åŠ¿
        val_acc = np.array(history["val_acc"])
        axes[1, 0].plot(val_acc, color='green', linewidth=3, label="Validation Accuracy")
        axes[1, 0].axhline(y=0.5, color='red', linestyle='--', alpha=0.7, label='Random')
        if len(val_acc) > 10:
            z = np.polyfit(range(len(val_acc)), val_acc, 1)
            trend = np.poly1d(z)(range(len(val_acc)))
            axes[1, 0].plot(trend, '--', alpha=0.8, linewidth=2, label=f'Trend ({z[0] * 100:.3f}%/epoch)')
        axes[1, 0].set_title("ğŸ¯ Validation Accuracy Trend", fontsize=14, fontweight='bold')
        axes[1, 0].set_xlabel("Epoch")
        axes[1, 0].set_ylabel("Accuracy")
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)

        # Losså·®å¼‚
        if len(history["train_loss"]) == len(history["val_loss"]):
            loss_diff = np.array(history["val_loss"]) - np.array(history["train_loss"])
            axes[1, 1].plot(loss_diff, color='purple', linewidth=2)
            axes[1, 1].axhline(y=0, color='black', linestyle='-', alpha=0.5)
            axes[1, 1].set_title("ğŸ“Š Overfitting Monitor\n(Val Loss - Train Loss)", fontsize=14, fontweight='bold')
            axes[1, 1].set_xlabel("Epoch")
            axes[1, 1].set_ylabel("Loss Difference")
            axes[1, 1].grid(True, alpha=0.3)

        # æ€§èƒ½æ€»ç»“
        axes[1, 2].axis('off')
        final_train_acc = history["train_acc"][-1] if history["train_acc"] else 0
        final_val_acc = history["val_acc"][-1] if history["val_acc"] else 0
        max_val_acc = max(history["val_acc"]) if history["val_acc"] else 0
        final_lr = history["lr"][-1] if history["lr"] else 0

        summary_text = f"""
        ğŸ“Š Training Summary
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        Final Train Acc: {final_train_acc:.4f}
        Final Val Acc: {final_val_acc:.4f}
        Best Val Acc: {max_val_acc:.4f}
        Final LR: {final_lr:.2e}

        ğŸ¯ vs Random Baseline
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        Improvement: {(max_val_acc - 0.5) * 100:+.1f}%

        ğŸ† Model Status
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        {'ğŸ‰ Excellent!' if max_val_acc > 0.52 else 'ğŸ‘ Good!' if max_val_acc > 0.51 else 'ğŸ“ˆ Needs Work'}
        """

        axes[1, 2].text(0.1, 0.5, summary_text, transform=axes[1, 2].transAxes,
                        fontsize=12, verticalalignment='center', fontfamily='monospace',
                        bbox=dict(boxstyle="round,pad=0.3", facecolor="lightblue", alpha=0.7))

        plt.tight_layout()

        # ä¿å­˜å›¾åƒ
        try:
            plt.savefig(save_path, dpi=150, bbox_inches='tight', facecolor='white')
            logger.success(f"âœ… Training plots saved to: {save_path}")
        except Exception as e:
            logger.warning(f"âš ï¸  Could not save plot: {e}")

        # æ˜¾ç¤ºå›¾åƒ
        try:
            plt.show()
        except Exception as e:
            logger.warning(f"âš ï¸  Could not display plot: {e}")
        finally:
            plt.close(fig)

    except Exception as e:
        logger.error(f"âŒ Plotting failed: {e}")
        logger.info("ğŸ“Š Printing text summary instead...")
        print_text_summary(history)


def print_text_summary(history: dict):
    """æ–‡æœ¬å½¢å¼çš„è®­ç»ƒæ€»ç»“"""
    logger.info("ğŸ“Š TRAINING SUMMARY (Text Format)")
    logger.info("=" * 50)
    if history.get("train_loss"):
        logger.info(f"Final train loss: {history['train_loss'][-1]:.4f}")
        logger.info(f"Final val loss: {history['val_loss'][-1]:.4f}")
    if history.get("train_acc"):
        logger.info(f"Final train acc: {history['train_acc'][-1]:.4f}")
        logger.info(f"Final val acc: {history['val_acc'][-1]:.4f}")
        logger.info(f"Best val acc: {max(history['val_acc']):.4f}")
    if history.get("lr"):
        logger.info(f"Final learning rate: {history['lr'][-1]:.2e}")
    logger.info("=" * 50)


# =========================
# ä¸»æ‰§è¡Œå‡½æ•°
# =========================
def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    # è®¾ç½®æ—¥å¿—
    log_file = setup_logger()
    logger.info("ğŸš€ Enhanced Inception Stock CNN Started")
    logger.info("=" * 80)

    set_seed(42)

    # æ•°æ®æ–‡ä»¶è·¯å¾„ - è¯·ä¿®æ”¹ä¸ºæ‚¨çš„CSVæ–‡ä»¶è·¯å¾„
    file_path = r"E:\Vesper\data_downloader\data\raw\indexData\HS300_daily_kline.csv"
    asset_code = None  # å¦‚æœæ˜¯å¤šèµ„äº§CSVï¼Œè®¾ç½®èµ„äº§ä»£ç 

    logger.info("ğŸ“Š Loading market data...")
    try:
        df = load_generic_market_csv(file_path, asset_code=asset_code)
        logger.success(f"âœ… Loaded {len(df)} rows")
        logger.info(f"ğŸ“… Date range: {df['date'].min()} to {df['date'].max()}")
        logger.info(f"ğŸ’° Price range: {df['close'].min():.2f} - {df['close'].max():.2f}")
    except Exception as e:
        logger.error(f"âŒ Failed to load data: {e}")
        return

    # å¿«é€Ÿæ•°æ®æ£€æŸ¥
    logger.info("ğŸ” Data quality check:")
    logger.info(f"   Missing close values: {df['close'].isna().sum()}")
    logger.info(f"   Missing volume values: {df['volume'].isna().sum()}")
    logger.info(f"   Zero volume days: {(df['volume'] == 0).sum()}")

    # é…ç½®æµ‹è¯• - æµ‹è¯•ä¸åŒçš„é¢„æµ‹æ¨¡å¼
    configs_to_test = [
        {
            "name": "ğŸ¯ Adaptive Classification + Inception",
            "config": EnhancedInceptionConfig(
                window_size=30,
                inception_layers=3,
                base_channels=32,
                dropout=0.2,
                prediction_mode="adaptive_classification",
                threshold_multiplier=0.5,
                epochs=60,
                lr=3e-4,
                use_class_weights=True,
                use_data_augmentation=True
            )
        },
        {
            "name": "ğŸ“ˆ Simple Binary + Inception",
            "config": EnhancedInceptionConfig(
                window_size=25,
                inception_layers=2,
                base_channels=24,
                dropout=0.15,
                prediction_mode="simple_binary",
                epochs=50,
                lr=4e-4,
                use_class_weights=True,
                use_data_augmentation=False
            )
        }
    ]

    results = {}

    for i, config_test in enumerate(configs_to_test):
        logger.info("=" * 80)
        logger.info(f"ğŸ§ª Test {i + 1}/{len(configs_to_test)}: {config_test['name']}")
        logger.info("=" * 80)

        try:
            # åˆå§‹åŒ–æµæ°´çº¿
            cfg = config_test["config"]
            pipe = EnhancedInceptionStockCNN(cfg)

            # ç‰¹å¾å·¥ç¨‹
            logger.info("ğŸ”§ Starting feature engineering...")
            df_clean = pipe.prepare_dataframe(df, target_col="close")

            if len(df_clean) < 500:
                logger.warning(f"âš ï¸  Too few samples ({len(df_clean)}) after preprocessing!")
                continue

            logger.success(f"âœ… Preprocessed dataset: {len(df_clean)} samples")

            # è®­ç»ƒæ¨¡å‹
            logger.info("ğŸ‹ï¸  Starting model training...")
            history, (y_true, y_pred, confidences) = pipe.fit(df_clean)

            # å­˜å‚¨ç»“æœ
            acc = accuracy_score(y_true, y_pred)
            f1 = f1_score(y_true, y_pred, average="macro")

            results[config_test["name"]] = {
                "accuracy": acc,
                "f1": f1,
                "history": history,
                "predictions": (y_true, y_pred, confidences),
                "config": cfg
            }

            # ç»˜åˆ¶ç»“æœ
            logger.info("ğŸ“Š Generating training plots...")
            plot_filename = f"inception_training_{i + 1}.png"
            try:
                plot_enhanced_inception_history(history, plot_filename)
            except Exception as plot_err:
                logger.warning(f"âš ï¸  Plotting failed: {plot_err}")
                print_text_summary(history)

        except Exception as e:
            logger.error(f"âŒ Error in {config_test['name']}: {str(e)}")
            import traceback
            logger.error(f"Stack trace: {traceback.format_exc()}")
            continue

    # æœ€ç»ˆæ¯”è¾ƒ
    if results:
        logger.success("=" * 80)
        logger.success("ğŸ† FINAL COMPARISON")
        logger.success("=" * 80)

        best_result = None
        best_score = 0

        for name, result in results.items():
            acc = result['accuracy']
            f1 = result['f1']
            improvement = (acc - 0.5) * 100

            logger.success(f"ğŸ“Š {name}")
            logger.info(f"   Accuracy: {acc:.4f}")
            logger.info(f"   F1-macro: {f1:.4f}")
            logger.info(f"   vs Random: {improvement:+.1f}%")

            # è®¡ç®—ç»¼åˆå¾—åˆ†
            composite_score = acc * 0.7 + f1 * 0.3
            logger.info(f"   Composite Score: {composite_score:.4f}")

            if composite_score > best_score:
                best_score = composite_score
                best_result = (name, result)

            logger.info("")

        # æœ€ä½³æ¨¡å‹
        if best_result:
            best_name, best_data = best_result
            logger.success(f"ğŸ¥‡ BEST MODEL: {best_name}")
            logger.success(f"   Accuracy: {best_data['accuracy']:.4f}")
            logger.success(f"   F1-Score: {best_data['f1']:.4f}")
            logger.success(f"   Composite: {best_score:.4f}")

            # æ€§èƒ½è¯„çº§
            acc = best_data['accuracy']
            if acc > 0.55:
                logger.success("ğŸ‰ OUTSTANDING! This is excellent for stock prediction!")
            elif acc > 0.52:
                logger.success("ğŸ‰ EXCELLENT! Very good predictive performance!")
            elif acc > 0.51:
                logger.success("ğŸ‘ GOOD! Clear predictive signal detected!")
            elif acc > 0.505:
                logger.info("ğŸ“ˆ MODERATE: Some signal, needs optimization.")
            else:
                logger.warning("ğŸ“Š WEAK: Consider different features/approach.")

            # ä¿å­˜æœ€ä½³æ¨¡å‹é…ç½®
            logger.info("ğŸ’¾ Best model configuration:")
            best_cfg = best_data['config']
            logger.info(f"   Window size: {best_cfg.window_size}")
            logger.info(f"   Inception layers: {best_cfg.inception_layers}")
            logger.info(f"   Base channels: {best_cfg.base_channels}")
            logger.info(f"   Dropout: {best_cfg.dropout}")
            logger.info(f"   Learning rate: {best_cfg.lr}")
            logger.info(f"   Prediction mode: {best_cfg.prediction_mode}")

    else:
        logger.error("âŒ No successful runs completed!")

    logger.success("=" * 80)
    logger.success("âœ… ENHANCED INCEPTION STOCK CNN ANALYSIS COMPLETE!")
    logger.success("=" * 80)

    logger.info("ğŸ¯ Key Achievements:")
    logger.info("  âœ… Multi-scale Inception architecture")
    logger.info("  âœ… Volume-based feature engineering")
    logger.info("  âœ… Adaptive threshold labeling")
    logger.info("  âœ… Comprehensive logging with loguru")
    logger.info("  âœ… Advanced data quality analysis")
    logger.info("  âœ… Confidence-based predictions")
    logger.info("  âœ… Early stopping & model persistence")

    logger.info(f"ğŸ“‹ Log file saved to: {log_file}")
    logger.success("=" * 80)


if __name__ == "__main__":
    main()